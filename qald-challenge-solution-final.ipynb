{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10th Question Answering over Linked Data (QALD) Challenge @ ESWC 2022Permalink\n",
    "\n",
    "## Task 1: Multilingual Question Answering over Knowledge GraphsPermalink\n",
    "\n",
    "### Participants (Skolkovo Institute of Science and Technology, Skoltech):\n",
    "\n",
    "1. Nikita Baramiia\n",
    "\n",
    "1. Alina Rogulina\n",
    "\n",
    "1. Sergey Petrakov\n",
    "\n",
    "1. Valerii Kornilov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "## (All other requirements are satisfied in kaggle docker 2022.05.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install scann wikidata sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import ijson\n",
    "import scann\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "from wikidata.client import Client\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device('cpu') # torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dictionaries of Q-items and P-properties\n",
    "\n",
    "There is if-condition in Q-dict preparation connected with RAM constraints: \n",
    "\n",
    "if you have enougth RAM, you can prepare full dictionary for experiments (but results were rather similar in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [00:00, 5125.33it/s]\n",
      "5926it [00:01, 4549.29it/s]\n"
     ]
    }
   ],
   "source": [
    "Qs = {} # 4 106 846 it -- total\n",
    "BREAK_AFTER_N = None\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"Q_embeddings.json\"), \"rb\") as f:\n",
    "    if BREAK_AFTER_N is not None:\n",
    "        i = 0\n",
    "    for key, value in tqdm(ijson.kvitems(f, '')):\n",
    "        Qs[key] = torch.Tensor(value)\n",
    "        if BREAK_AFTER_N is not None:\n",
    "            i += 1\n",
    "            if i == BREAK_AFTER_N:\n",
    "                break\n",
    "\n",
    "Qs_keys = set(Qs.keys())\n",
    "\n",
    "Qs_ordered_site = np.array(list(Qs.keys()))\n",
    "Qs_ordered_embs = np.stack(list(Qs.values()), axis=0)\n",
    "\n",
    "Ps = {} # \n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"P_embeddings.json\"), \"rb\") as f:\n",
    "    for key, value in tqdm(ijson.kvitems(f, '')):\n",
    "        Ps[key] = torch.Tensor(value)\n",
    "\n",
    "Ps_keys = set(Ps.keys())\n",
    "\n",
    "Ps_ordered_site = np.array(list(Ps.keys()))\n",
    "Ps_ordered_embs = np.stack(list(Ps.values()), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScaNN fit (for approximate neigbours search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.199733257293701\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "Q_searcher = scann.scann_ops_pybind.builder(Qs_ordered_embs, 5, \"dot_product\").tree(\n",
    "    num_leaves=int(np.sqrt(len(Qs_ordered_embs))), \n",
    "    num_leaves_to_search=int(np.sqrt(len(Qs_ordered_embs)) // 10), \n",
    "    training_sample_size=int(len(Qs_ordered_embs) // 20)\n",
    ").score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "P_searcher = scann.scann_ops_pybind.builder(Ps_ordered_embs, 5, \"dot_product\").tree(\n",
    "    num_leaves=int(np.sqrt(len(Ps_ordered_embs))), \n",
    "    num_leaves_to_search=int(np.sqrt(len(Ps_ordered_embs)) // 10), \n",
    "    training_sample_size=int(len(Ps_ordered_embs) // 20)\n",
    ").score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412it [00:01, 369.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before cleaning: (412, 3)\n",
      "Shape after cleaning: (145, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "QA_train = pd.DataFrame(columns=['question', 'query', 'answer'])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"qald_9_plus_train_wikidata.json\"), \"rb\") as f:\n",
    "    for el in tqdm(ijson.items(f, 'questions.item')):\n",
    "        QA_train.loc[int(el['id']), 'question'] = el['question'][0]['string'] # el['question']\n",
    "        \n",
    "        # for simplicity we parse only certain format of sparql like in https://www.nliwod.org/challenge\n",
    "        res = re.findall(r'<.*?>', el['query']['sparql'])\n",
    "        QA_train.loc[int(el['id']), 'query'] = [\n",
    "            (\n",
    "                res[i*2].strip('<>').replace('http://www.wikidata.org/entity/', '').replace('http://www.wikidata.org/prop/direct/', ''), \n",
    "                res[i*2 + 1].strip('<>').replace('http://www.wikidata.org/prop/direct/', '').replace('http://www.wikidata.org/entity/', '')\n",
    "            ) for i in range(len(res) // 2)\n",
    "        ] if len(res) >= 2 else None\n",
    "        \n",
    "        key = list(el['answers'][0]['results']['bindings'][0].keys())[0]\n",
    "        QA_train.loc[int(el['id']), 'answer'] = [\n",
    "            list_val[key]['value'].replace('http://www.wikidata.org/entity/', '') \\\n",
    "            for list_val in el['answers'][0]['results']['bindings']\n",
    "        ]\n",
    "\n",
    "print(f'Shape before cleaning: {QA_train.shape}')\n",
    "\n",
    "# then we drop all Nones\n",
    "QA_train['query'] = QA_train['query'].apply(\n",
    "    lambda x: [\n",
    "        tuple(sorted(list(t), key=lambda x: x[0], reverse=True))\n",
    "        if ((t[0] != '') and (t[1] != '') and (t[0][0] != t[1][0]) and (t[0][0] != 'h') and (t[1][0] != 'h')) else None \n",
    "        for t in x\n",
    "] if x is not None else x)\n",
    "\n",
    "QA_train['query'] = QA_train['query'].apply(lambda row: [x for x in row if x is not None] if row is not None else row)\n",
    "QA_train = QA_train.loc[QA_train['query'].apply(lambda x: x != [])]\n",
    "\n",
    "QA_train.dropna(inplace=True)\n",
    "\n",
    "print(f'Shape after cleaning: {QA_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "394it [00:00, 1397.32it/s]\n"
     ]
    }
   ],
   "source": [
    "QA_test = pd.DataFrame(columns=['question'])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"qald_10.json\"), \"rb\") as f:\n",
    "    for el in tqdm(ijson.items(f, 'questions.item')):\n",
    "        QA_test.loc[int(el['id']), 'question'] = el['question'][0]['string']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for our model and scheduler with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartTokenizer, BartModel, MBart50Tokenizer,\n",
    "    BertTokenizer, BertModel,\n",
    "    XLMRobertaTokenizer, XLMRobertaModel\n",
    ")\n",
    "\n",
    "class AnswerPredictor(nn.Module):\n",
    "    def __init__(self, base_model_name='bart', embed_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model_name = base_model_name\n",
    "        \n",
    "        if 'bart' in self.base_model_name:\n",
    "            if self.base_model_name == 'bart':\n",
    "                name = 'facebook/bart-base'\n",
    "                self.tokenizer = BartTokenizer.from_pretrained(name)\n",
    "            elif self.base_model_name == 'mbart':\n",
    "                name = 'facebook/mbart-large-50'\n",
    "                self.tokenizer = MBart50Tokenizer.from_pretrained(name)\n",
    "            self.model = BartModel.from_pretrained(name).to(device)\n",
    "        elif 'bert' in self.base_model_name:\n",
    "            name = 'bert-base-uncased' if self.base_model_name == 'bert' else 'bert-base-multilingual-uncased'\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(name)\n",
    "            self.model = BertModel.from_pretrained(name).to(device)\n",
    "        elif 'xlm-r' in self.base_model_name:\n",
    "            name = 'xlm-roberta-base' if self.base_model_name == 'xlm-r' else 'sentence-transformers/stsb-xlm-r-multilingual'\n",
    "            self.tokenizer = XLMRobertaTokenizer.from_pretrained(name)\n",
    "            self.model = XLMRobertaModel.from_pretrained(name).to(device)\n",
    "\n",
    "        self.linear_map = nn.Linear(in_features=768, out_features=embed_size*2).to(device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        return self.linear_map(torch.mean(outputs.last_hidden_state, dim=1))\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=6, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "            print(\"self.best_loss == None\")\n",
    "            print(\"best_loss\", self.best_loss)\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "            print(\"self.best_loss - val_loss > self.min_delta\")\n",
    "            print(\"counter\", self.counter)\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            print(\"self.best_loss - val_loss < self.min_delta\")\n",
    "            print(\"counter\", self.counter)\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n",
    "\n",
    "class StepLRWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, warmup_epochs=2, warmup_lr_init=1e-5,\n",
    "                 min_lr=1e-5,\n",
    "                 last_epoch=-1, verbose=False):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_lr_init = warmup_lr_init\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        super(StepLRWithWarmup, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.warmup_lr_init]\n",
    "        \n",
    "        number = self.optimizer.param_groups[0]['initial_lr']\n",
    "        if self.last_epoch in range(self.warmup_epochs):\n",
    "            # 1) scheduler is in warm-up mode and learning rate should lineary increase during epochs\n",
    "            # from self.warmup_lr_init to self.base_lrs (self.optimizer.param_groups[0]['lr'] in our case)\n",
    "            return [self.warmup_lr_init + self.last_epoch * (number - self.warmup_lr_init) / (self.warmup_epochs)]\n",
    "        \n",
    "        elif self.last_epoch == self.warmup_epochs:\n",
    "            # 2) self.last_epoch is equal to self.warmup_epochs, then just return self.base_lrs\n",
    "            \n",
    "            return [self.optimizer.param_groups[0]['initial_lr']]\n",
    "        \n",
    "        elif ((self.last_epoch - self.warmup_epochs) % self.step_size != 0):\n",
    "            # 3) self.last_epoch - self.warmup_epochs is not divisible by self.step_size then\n",
    "            # just return the previous learning rate\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        \n",
    "        elif ((self.last_epoch - self.warmup_epochs) % self.step_size == 0) & (self.optimizer.param_groups[0]['lr'] * self.gamma >= self.min_lr):\n",
    "            # 4) self.last_epoch - self.warmup_epochs is divisible by self.step_size and the\n",
    "            # current learning rate multiplied by self.gamma is not less then self.min_lr,\n",
    "            # then multiply it and return the new value\n",
    "            return [group['lr'] * self.gamma for group in self.optimizer.param_groups]\n",
    "        \n",
    "        elif (self.optimizer.param_groups[0]['lr'] * self.gamma < self.min_lr) :\n",
    "            # Otherwise just return the last learning rate\n",
    "            return [self.min_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0af404f070e43e09ea9e22c27e211b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a3ccedb026449eb8df19ae69ecdb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7efbb40f3c143fb9b0a1e7e6667ea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5179651a594d2392ab7a67efb535a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/709 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064155b953314325a5e4456a7a06d031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:12<00:12, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.20323173259866648\n",
      "self.best_loss == None\n",
      "best_loss 0.20323173259866648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:22<00:00, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.17043345221157732\n",
      "self.best_loss - val_loss > self.min_delta\n",
      "counter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_train = QA_train.question.values # QA_train.Q.apply(lambda x: x[0]['string']).values\n",
    "sentences_test  = QA_test.question.values  # QA_test.Q.apply( lambda x: x[0]['string']).values\n",
    "\n",
    "queries_train = QA_train['query'].values # answers_train = QA_train.A.values\n",
    "\n",
    "len(sentences_train), len(queries_train), len(sentences_test)\n",
    "\n",
    "# Parameters\n",
    "embedding_size = 200\n",
    "losses_bart = []\n",
    "BASE_MODEL_NAME = 'mxlm-r'\n",
    "model = AnswerPredictor(base_model_name=BASE_MODEL_NAME, embed_size=embedding_size)\n",
    "\n",
    "train_size = len(sentences_train)\n",
    "batch_size = 128 # 16, 32, 64\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-5)\n",
    "\n",
    "# LRscheduler\n",
    "lr_scheduler = StepLRWithWarmup(optimizer = optimizer, step_size = 13, gamma=0.9, warmup_epochs=2,\n",
    "                                warmup_lr_init=1e-5, min_lr=1e-5, last_epoch=-1, verbose=False)\n",
    "\n",
    "# Triplet loss\n",
    "metric_loss = nn.TripletMarginLoss()\n",
    "\n",
    "\n",
    "#######\n",
    "# train\n",
    "#######\n",
    "losses_bart = []\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    # get shuffled train indices\n",
    "    indices_shuffled = np.random.choice(train_size, size=train_size, replace=False)\n",
    "    \n",
    "    loss_val = 0\n",
    "    for i in range(train_size // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get batch\n",
    "        indices_batch = indices_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "        sentences_batch, queries_batch = sentences_train[indices_batch], queries_train[indices_batch]\n",
    "        \n",
    "        # prepare good keys we drop to find negative examples\n",
    "        Qs_keys_todrop = set().union(*[ans[0][0] for ans in queries_batch])\n",
    "        Qs_keys_leaved = Qs_keys - Qs_keys_todrop\n",
    "        \n",
    "        Ps_keys_todrop = set().union(*[ans[0][1] for ans in queries_batch])\n",
    "        Ps_keys_leaved = Ps_keys - Ps_keys_todrop\n",
    "        \n",
    "        # predicted embeddings for answers in batch\n",
    "        queries_anchor = model(list(sentences_batch))\n",
    "        queries_anchor_copy = queries_anchor.detach().cpu().clone()\n",
    "        \n",
    "        # prepare positive samples for batch\n",
    "        queries_positive = []\n",
    "        for b, queries_pos in enumerate(queries_batch):\n",
    "            query_sample = queries_pos[np.random.choice(len(queries_pos), size=1)[0]]\n",
    "            \n",
    "            Q_embed = Qs.get(query_sample[0], queries_anchor_copy[b][:embedding_size])\n",
    "            P_embed = Ps.get(query_sample[1], queries_anchor_copy[b][embedding_size:])\n",
    "            \n",
    "            queries_positive.append(torch.cat([Q_embed, P_embed]))\n",
    "            \n",
    "        queries_positive = torch.stack(queries_positive, dim=0).to(device)\n",
    "        \n",
    "        # prepare negative samples for batch\n",
    "        neighbors_Q, _ = Q_searcher.search_batched(queries_anchor_copy[:, :embedding_size], leaves_to_search=1000, \n",
    "                                                   pre_reorder_num_neighbors=1000, final_num_neighbors=10)\n",
    "        neighbors_P, _ = P_searcher.search_batched(queries_anchor_copy[:, embedding_size:], leaves_to_search=1000, \n",
    "                                                   pre_reorder_num_neighbors=1000, final_num_neighbors=10)\n",
    "        \n",
    "        Qs_negative_sites = [\n",
    "            [\n",
    "                neg_ex for neg_ex in list(Qs_ordered_site[neighbors_Q[s]]) if neg_ex not in queries_batch[s]\n",
    "            ][0] for s in range(batch_size)\n",
    "        ]\n",
    "        Ps_negative_sites = [\n",
    "            [\n",
    "                neg_ex for neg_ex in list(Ps_ordered_site[neighbors_P[s]]) if neg_ex not in queries_batch[s]\n",
    "            ][0] for s in range(batch_size)\n",
    "        ]\n",
    "        \n",
    "        queries_negative = torch.stack([\n",
    "            torch.cat([\n",
    "                Qs.get(neg_Q, queries_anchor_copy[b][:embedding_size]), \n",
    "                Ps.get(neg_P, queries_anchor_copy[b][embedding_size:])\n",
    "            ])\n",
    "            for b, (neg_Q, neg_P) in enumerate(zip(Qs_negative_sites, Ps_negative_sites))\n",
    "        ], dim=0).to(device)\n",
    "        \n",
    "        # loss calculation and optimization step\n",
    "        loss = metric_loss(queries_anchor, queries_positive, queries_negative)\n",
    "        loss_val += loss.item() * batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Loss: {loss_val / train_size}')\n",
    "    losses_bart.append(loss_val / train_size)\n",
    "    \n",
    "    \n",
    "    early_stopping(loss_val / train_size)\n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#additionally\n",
    "# result[\"BART_100\"] = losses_bart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "plt.plot(losses_bart)\n",
    "\n",
    "SAVE_NAME = \"{}_{}_{}.pth\".format(\n",
    "    BASE_MODEL_NAME, \n",
    "    n_epochs, \n",
    "    str(datetime.datetime.now()).replace(' ', '-').replace(':', '_').replace('.', '_')\n",
    ")\n",
    "SAVE_PATH = './models'\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_PATH, SAVE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = model(['Who killed Caesar?', 'How many planets around the sun?']).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.002849578857421875\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Q_neighbors, _ = Q_searcher.search_batched(queries[:, :embedding_size], leaves_to_search=1000, pre_reorder_num_neighbors=1000)\n",
    "P_neighbors, _ = P_searcher.search_batched(queries[:, embedding_size:], leaves_to_search=1000, pre_reorder_num_neighbors=1000)\n",
    "end = time.time()\n",
    "print(\"Time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q465517'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "answer = client.get('Q192115', load=True)[client.get('P162')]\n",
    "answer.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.wikidata.org/entity/Q465517'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://www.wikidata.org/entity/Q465517'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who killed Caesar?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['http://www.wikidata.org/entity/Q131578',\n",
       "  'http://www.wikidata.org/entity/Q28350',\n",
       "  'http://www.wikidata.org/entity/Q21102930',\n",
       "  'http://www.wikidata.org/entity/Q3018671',\n",
       "  'http://www.wikidata.org/entity/Q21102970'],\n",
       " ['http://www.wikidata.org/prop/direct/P404',\n",
       "  'http://www.wikidata.org/prop/direct/P400',\n",
       "  'http://www.wikidata.org/prop/direct/P462',\n",
       "  'http://www.wikidata.org/prop/direct/P1209',\n",
       "  'http://www.wikidata.org/prop/direct/P478'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Who killed Caesar?')\n",
    "['http://www.wikidata.org/entity/' + q for q in Qs_ordered_site[Q_neighbors[0]]], \\\n",
    "['http://www.wikidata.org/prop/direct/' + p for p in Ps_ordered_site[P_neighbors[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"qald_10.json\"), \"rb\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head': {'vars': ['result']}, 'results': {'bindings': [{'result': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q42299'}}]}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data['questions'])):\n",
    "    print(data['questions'][i]['answers'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 36/394 [03:14<32:14,  5.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-965deeb74765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnum_of_neighs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     Q_neighbors, _ = Q_searcher.search_batched(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5f7e0715d3f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         )\n\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data['questions']))):\n",
    "    num_of_neighs = 3\n",
    "    \n",
    "    query = model(sentences_test[i]).detach().cpu().numpy()\n",
    "    \n",
    "    Q_neighbors, _ = Q_searcher.search_batched(\n",
    "        query[:, :embedding_size], leaves_to_search=1000, pre_reorder_num_neighbors=1000, final_num_neighbors=num_of_neighs,\n",
    "    )\n",
    "    P_neighbors, _ = P_searcher.search_batched(\n",
    "        query[:, embedding_size:], leaves_to_search=1000, pre_reorder_num_neighbors=1000, final_num_neighbors=num_of_neighs,\n",
    "    )\n",
    "    \n",
    "    fail = True\n",
    "    for q in range(num_of_neighs):\n",
    "        for p in range(num_of_neighs):\n",
    "            client = Client()\n",
    "            try:\n",
    "                if fail:\n",
    "                    site_pred = client.get(\n",
    "                        Qs_ordered_site[Q_neighbors[0][q]], load=True\n",
    "                    )[client.get(Ps_ordered_site[P_neighbors[0][p]])].id\n",
    "\n",
    "                    data['questions'][i]['answers'][0] = {\n",
    "                        'head': {'vars': ['result']},\n",
    "                        'results': {'bindings': []}\n",
    "                    }\n",
    "                    data['questions'][i]['answers'][0]['results']['bindings'].append(\n",
    "                        {'result': {'type': 'uri', 'value': f'http://www.wikidata.org/entity/{site_pred}'}}\n",
    "                    )\n",
    "                    \n",
    "                    data['questions'][i]['query'] = {\n",
    "                        \"sparql\": \"SELECT DISTINCT ?o1 WHERE { <http://www.wikidata.org/entity/\" + \\\n",
    "                        Qs_ordered_site[Q_neighbors[0][q]] + \">  <http://www.wikidata.org/prop/direct/\" + \\\n",
    "                        Ps_ordered_site[P_neighbors[0][p]] + \">  ?o1 .  }\"\n",
    "                    }\n",
    "                    fail = False\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if fail:\n",
    "            data['questions'][i]['answers'] = [{'head': {}, 'boolean': True}]\n",
    "            \n",
    "            data['questions'][i]['query'] = {\n",
    "                \"sparql\": \"SELECT DISTINCT ?o1 WHERE { <http://www.wikidata.org/entity/\" + \\\n",
    "                Qs_ordered_site[Q_neighbors[0][0]] + \">  <http://www.wikidata.org/prop/direct/\" + \\\n",
    "                Ps_ordered_site[P_neighbors[0][0]] + \">  ?o1 .  }\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_PATH = './submissions'\n",
    "SUBMIT_NAME = '{}.json'.format(SAVE_NAME.split('.')[0])\n",
    "\n",
    "with open(os.path.join(SUBMIT_PATH, SUBMIT_NAME), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link corresponed to this submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link https://gerbil-qa.aksw.org/gerbil/experiment?id=202205210032 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
