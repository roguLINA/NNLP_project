{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10th Question Answering over Linked Data (QALD) Challenge @ ESWC 2022Permalink\n",
    "\n",
    "## Task 1: Multilingual Question Answering over Knowledge GraphsPermalink\n",
    "\n",
    "### Participants (Skolkovo Institute of Science and Technology, Skoltech):\n",
    "\n",
    "1. Nikita Baramiia\n",
    "\n",
    "1. Alina Rogulina\n",
    "\n",
    "1. Sergey Petrakov\n",
    "\n",
    "1. Valerii Kornilov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "## (All other requirements are satisfied in kaggle docker 2022.05.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install scann wikidata sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import ijson\n",
    "import scann\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "from wikidata.client import Client\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device('cpu') # torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dictionaries of Q-items and P-properties\n",
    "\n",
    "There is if-condition in Q-dict preparation connected with RAM constraints: \n",
    "\n",
    "if you have enougth RAM, you can prepare full dictionary for experiments (but results were rather similar in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24787it [00:05, 4388.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-420d10f2cf89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mBREAK_AFTER_N\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mijson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkvitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mQs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mBREAK_AFTER_N\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Qs = {} # 4 106 846 it -- total\n",
    "BREAK_AFTER_N = None\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"Q_embeddings.json\"), \"rb\") as f:\n",
    "    if BREAK_AFTER_N is not None:\n",
    "        i = 0\n",
    "    for key, value in tqdm(ijson.kvitems(f, '')):\n",
    "        Qs[key] = torch.Tensor(value)\n",
    "        if BREAK_AFTER_N is not None:\n",
    "            i += 1\n",
    "            if i == BREAK_AFTER_N:\n",
    "                break\n",
    "\n",
    "Qs_keys = set(Qs.keys())\n",
    "\n",
    "Qs_ordered_site = np.array(list(Qs.keys()))\n",
    "Qs_ordered_embs = np.stack(list(Qs.values()), axis=0)\n",
    "\n",
    "Ps = {} # \n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"P_embeddings.json\"), \"rb\") as f:\n",
    "    for key, value in tqdm(ijson.kvitems(f, '')):\n",
    "        Ps[key] = torch.Tensor(value)\n",
    "\n",
    "Ps_keys = set(Ps.keys())\n",
    "\n",
    "Ps_ordered_site = np.array(list(Ps.keys()))\n",
    "Ps_ordered_embs = np.stack(list(Ps.values()), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScaNN fit (for approximate neigbours search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3.7140331268310547\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "Q_searcher = scann.scann_ops_pybind.builder(Qs_ordered_embs, 5, \"dot_product\").tree(\n",
    "    num_leaves=int(np.sqrt(len(Qs_ordered_embs))), \n",
    "    num_leaves_to_search=int(np.sqrt(len(Qs_ordered_embs)) // 10), \n",
    "    training_sample_size=int(len(Qs_ordered_embs) // 20)\n",
    ").score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "P_searcher = scann.scann_ops_pybind.builder(Ps_ordered_embs, 5, \"dot_product\").tree(\n",
    "    num_leaves=int(np.sqrt(len(Ps_ordered_embs))), \n",
    "    num_leaves_to_search=int(np.sqrt(len(Ps_ordered_embs)) // 10), \n",
    "    training_sample_size=int(len(Ps_ordered_embs) // 20)\n",
    ").score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412it [00:01, 389.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before cleaning: (412, 3)\n",
      "Shape after cleaning: (145, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "QA_train = pd.DataFrame(columns=['question', 'query', 'answer'])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"qald_9_plus_train_wikidata.json\"), \"rb\") as f:\n",
    "    for el in tqdm(ijson.items(f, 'questions.item')):\n",
    "        QA_train.loc[int(el['id']), 'question'] = el['question'][0]['string'] # el['question']\n",
    "        \n",
    "        # for simplicity we parse only certain format of sparql like in https://www.nliwod.org/challenge\n",
    "        res = re.findall(r'<.*?>', el['query']['sparql'])\n",
    "        QA_train.loc[int(el['id']), 'query'] = [\n",
    "            (\n",
    "                res[i*2].strip('<>').replace('http://www.wikidata.org/entity/', '').replace('http://www.wikidata.org/prop/direct/', ''), \n",
    "                res[i*2 + 1].strip('<>').replace('http://www.wikidata.org/prop/direct/', '').replace('http://www.wikidata.org/entity/', '')\n",
    "            ) for i in range(len(res) // 2)\n",
    "        ] if len(res) >= 2 else None\n",
    "        \n",
    "        key = list(el['answers'][0]['results']['bindings'][0].keys())[0]\n",
    "        QA_train.loc[int(el['id']), 'answer'] = [\n",
    "            list_val[key]['value'].replace('http://www.wikidata.org/entity/', '') \\\n",
    "            for list_val in el['answers'][0]['results']['bindings']\n",
    "        ]\n",
    "\n",
    "print(f'Shape before cleaning: {QA_train.shape}')\n",
    "\n",
    "# then we drop all Nones\n",
    "QA_train['query'] = QA_train['query'].apply(\n",
    "    lambda x: [\n",
    "        tuple(sorted(list(t), key=lambda x: x[0], reverse=True))\n",
    "        if ((t[0] != '') and (t[1] != '') and (t[0][0] != t[1][0]) and (t[0][0] != 'h') and (t[1][0] != 'h')) else None \n",
    "        for t in x\n",
    "] if x is not None else x)\n",
    "\n",
    "QA_train['query'] = QA_train['query'].apply(lambda row: [x for x in row if x is not None] if row is not None else row)\n",
    "QA_train = QA_train.loc[QA_train['query'].apply(lambda x: x != [])]\n",
    "\n",
    "QA_train.dropna(inplace=True)\n",
    "\n",
    "print(f'Shape after cleaning: {QA_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [00:00, 890.30it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QA_test = pd.DataFrame(columns=['question'])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"qald_10.json\"), \"rb\") as f:\n",
    "    for el in tqdm(ijson.items(f, 'questions.item')):\n",
    "        QA_test.loc[int(el['id']), 'question'] = el['question'][0]['string']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for our model and scheduler with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartTokenizer, BartModel, MBart50Tokenizer,\n",
    "    BertTokenizer, BertModel,\n",
    "    XLMRobertaTokenizer, XLMRobertaModel\n",
    ")\n",
    "\n",
    "class AnswerPredictor(nn.Module):\n",
    "    def __init__(self, base_model_name='bart', embed_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model_name = base_model_name\n",
    "        \n",
    "        if 'bart' in self.base_model_name:\n",
    "            if self.base_model_name == 'bart':\n",
    "                name = 'facebook/bart-base'\n",
    "                self.tokenizer = BartTokenizer.from_pretrained(name)\n",
    "            elif self.base_model_name == 'mbart':\n",
    "                name = 'facebook/mbart-large-50'\n",
    "                self.tokenizer = MBart50Tokenizer.from_pretrained(name)\n",
    "            self.model = BartModel.from_pretrained(name).to(device)\n",
    "        elif 'bert' in self.base_model_name:\n",
    "            name = 'bert-base-uncased' if self.base_model_name == 'bert' else 'bert-base-multilingual-uncased'\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(name)\n",
    "            self.model = BertModel.from_pretrained(name).to(device)\n",
    "        elif self.base_model_name == 'xlm-r':\n",
    "            name = 'xlm-roberta-base' if self.base_model_name == 'xlm-r' else 'sentence-transformers/stsb-xlm-r-multilingual'\n",
    "            self.tokenizer = XLMRobertaTokenizer.from_pretrained(name)\n",
    "            self.model = XLMRobertaModel.from_pretrained(name).to(device)\n",
    "\n",
    "        self.linear_map = nn.Linear(in_features=768, out_features=embed_size*2).to(device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.tokenizer(inputs, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        return self.linear_map(torch.mean(outputs.last_hidden_state, dim=1))\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=6, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "            print(\"self.best_loss == None\")\n",
    "            print(\"best_loss\", self.best_loss)\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "            print(\"self.best_loss - val_loss > self.min_delta\")\n",
    "            print(\"counter\", self.counter)\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            print(\"self.best_loss - val_loss < self.min_delta\")\n",
    "            print(\"counter\", self.counter)\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n",
    "\n",
    "class StepLRWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, warmup_epochs=2, warmup_lr_init=1e-5,\n",
    "                 min_lr=1e-5,\n",
    "                 last_epoch=-1, verbose=False):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_lr_init = warmup_lr_init\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        super(StepLRWithWarmup, self).__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.warmup_lr_init]\n",
    "        \n",
    "        number = self.optimizer.param_groups[0]['initial_lr']\n",
    "        if self.last_epoch in range(self.warmup_epochs):\n",
    "            # 1) scheduler is in warm-up mode and learning rate should lineary increase during epochs\n",
    "            # from self.warmup_lr_init to self.base_lrs (self.optimizer.param_groups[0]['lr'] in our case)\n",
    "            return [self.warmup_lr_init + self.last_epoch * (number - self.warmup_lr_init) / (self.warmup_epochs)]\n",
    "        \n",
    "        elif self.last_epoch == self.warmup_epochs:\n",
    "            # 2) self.last_epoch is equal to self.warmup_epochs, then just return self.base_lrs\n",
    "            \n",
    "            return [self.optimizer.param_groups[0]['initial_lr']]\n",
    "        \n",
    "        elif ((self.last_epoch - self.warmup_epochs) % self.step_size != 0):\n",
    "            # 3) self.last_epoch - self.warmup_epochs is not divisible by self.step_size then\n",
    "            # just return the previous learning rate\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "        \n",
    "        elif ((self.last_epoch - self.warmup_epochs) % self.step_size == 0) & (self.optimizer.param_groups[0]['lr'] * self.gamma >= self.min_lr):\n",
    "            # 4) self.last_epoch - self.warmup_epochs is divisible by self.step_size and the\n",
    "            # current learning rate multiplied by self.gamma is not less then self.min_lr,\n",
    "            # then multiply it and return the new value\n",
    "            return [group['lr'] * self.gamma for group in self.optimizer.param_groups]\n",
    "        \n",
    "        elif (self.optimizer.param_groups[0]['lr'] * self.gamma < self.min_lr) :\n",
    "            # Otherwise just return the last learning rate\n",
    "            return [self.min_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences_train = QA_train.question.values # QA_train.Q.apply(lambda x: x[0]['string']).values\n",
    "sentences_test  = QA_test.question.values  # QA_test.Q.apply( lambda x: x[0]['string']).values\n",
    "\n",
    "queries_train = QA_train['query'].values # answers_train = QA_train.A.values\n",
    "\n",
    "len(sentences_train), len(queries_train), len(sentences_test)\n",
    "\n",
    "# Parameters\n",
    "embedding_size = 200\n",
    "losses_bart = []\n",
    "BASE_MODEL_NAME = 'bart'\n",
    "model = AnswerPredictor(base_model_name=BASE_MODEL_NAME, embed_size=embedding_size)\n",
    "\n",
    "train_size = len(sentences_train)\n",
    "batch_size = 128 # 16, 32, 64\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-5)\n",
    "\n",
    "# LRscheduler\n",
    "lr_scheduler = StepLRWithWarmup(optimizer = optimizer, step_size = 13, gamma=0.9, warmup_epochs=2,\n",
    "                                warmup_lr_init=1e-5, min_lr=1e-5, last_epoch=-1, verbose=False)\n",
    "\n",
    "# Triplet loss\n",
    "metric_loss = nn.TripletMarginLoss()\n",
    "\n",
    "\n",
    "#######\n",
    "# train\n",
    "#######\n",
    "losses_bart = []\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    # get shuffled train indices\n",
    "    indices_shuffled = np.random.choice(train_size, size=train_size, replace=False)\n",
    "    \n",
    "    loss_val = 0\n",
    "    for i in range(train_size // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get batch\n",
    "        indices_batch = indices_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "        sentences_batch, queries_batch = sentences_train[indices_batch], queries_train[indices_batch]\n",
    "        \n",
    "        # prepare good keys we drop to find negative examples\n",
    "        Qs_keys_todrop = set().union(*[ans[0][0] for ans in queries_batch])\n",
    "        Qs_keys_leaved = Qs_keys - Qs_keys_todrop\n",
    "        \n",
    "        Ps_keys_todrop = set().union(*[ans[0][1] for ans in queries_batch])\n",
    "        Ps_keys_leaved = Ps_keys - Ps_keys_todrop\n",
    "        \n",
    "        # predicted embeddings for answers in batch\n",
    "        queries_anchor = model(list(sentences_batch))\n",
    "        queries_anchor_copy = queries_anchor.detach().cpu().clone()\n",
    "        \n",
    "        # prepare positive samples for batch\n",
    "        queries_positive = []\n",
    "        for b, queries_pos in enumerate(queries_batch):\n",
    "            query_sample = queries_pos[np.random.choice(len(queries_pos), size=1)[0]]\n",
    "            \n",
    "            Q_embed = Qs.get(query_sample[0], queries_anchor_copy[b][:embedding_size])\n",
    "            P_embed = Ps.get(query_sample[1], queries_anchor_copy[b][embedding_size:])\n",
    "            \n",
    "            queries_positive.append(torch.cat([Q_embed, P_embed]))\n",
    "            \n",
    "        queries_positive = torch.stack(queries_positive, dim=0).to(device)\n",
    "        \n",
    "        # prepare negative samples for batch\n",
    "        neighbors_Q, _ = Q_searcher.search_batched(queries_anchor_copy[:, :embedding_size], leaves_to_search=1000, \n",
    "                                                   pre_reorder_num_neighbors=1000, final_num_neighbors=10)\n",
    "        neighbors_P, _ = P_searcher.search_batched(queries_anchor_copy[:, embedding_size:], leaves_to_search=1000, \n",
    "                                                   pre_reorder_num_neighbors=1000, final_num_neighbors=10)\n",
    "        \n",
    "        Qs_negative_sites = [\n",
    "            [\n",
    "                neg_ex for neg_ex in list(Qs_ordered_site[neighbors_Q[s]]) if neg_ex not in queries_batch[s]\n",
    "            ][0] for s in range(batch_size)\n",
    "        ]\n",
    "        Ps_negative_sites = [\n",
    "            [\n",
    "                neg_ex for neg_ex in list(Ps_ordered_site[neighbors_P[s]]) if neg_ex not in queries_batch[s]\n",
    "            ][0] for s in range(batch_size)\n",
    "        ]\n",
    "        \n",
    "        queries_negative = torch.stack([\n",
    "            torch.cat([\n",
    "                Qs.get(neg_Q, queries_anchor_copy[b][:embedding_size]), \n",
    "                Ps.get(neg_P, queries_anchor_copy[b][embedding_size:])\n",
    "            ])\n",
    "            for b, (neg_Q, neg_P) in enumerate(zip(Qs_negative_sites, Ps_negative_sites))\n",
    "        ], dim=0).to(device)\n",
    "        \n",
    "        # loss calculation and optimization step\n",
    "        loss = metric_loss(queries_anchor, queries_positive, queries_negative)\n",
    "        loss_val += loss.item() * batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Loss: {loss_val / train_size}')\n",
    "    losses_bart.append(loss_val / train_size)\n",
    "    \n",
    "    \n",
    "    early_stopping(loss_val / train_size)\n",
    "    if early_stopping.early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additionally\n",
    "# result[\"BART_100\"] = losses_bart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "plt.plot(losses_bart)\n",
    "\n",
    "SAVE_NAME = \"{}_{}_{}.pth\".format(\n",
    "    BASE_MODEL_NAME, \n",
    "    n_epochs, \n",
    "    str(datetime.datetime.now()).replace(' ', '-').replace(':', '_').replace('.', '_')\n",
    ")\n",
    "SAVE_PATH = './models'\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_PATH, SAVE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = model(['Who killed Caesar?', 'How many planets around the sun?']).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Q_neighbors, _ = Q_searcher.search_batched(queries[:, :embedding_size], leaves_to_search=1000, pre_reorder_num_neighbors=1000)\n",
    "P_neighbors, _ = P_searcher.search_batched(queries[:, embedding_size:], leaves_to_search=1000, pre_reorder_num_neighbors=1000)\n",
    "end = time.time()\n",
    "print(\"Time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "answer = client.get('Q192115', load=True)[client.get('P162')]\n",
    "answer.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://www.wikidata.org/entity/Q465517'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Who killed Caesar?')\n",
    "['http://www.wikidata.org/entity/' + q for q in Qs_ordered_site[Q_neighbors[0]]], \\\n",
    "['http://www.wikidata.org/prop/direct/' + p for p in Ps_ordered_site[P_neighbors[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"qald_10.json\"), \"rb\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['questions'])):\n",
    "    print(data['questions'][i]['answers'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(data['questions']))):\n",
    "    num_of_neighs = 3\n",
    "    \n",
    "    query = model(sentences_test[i]).detach().cpu().numpy()\n",
    "    \n",
    "    Q_neighbors, _ = Q_searcher.search_batched(\n",
    "        query[:, :embedding_size], leaves_to_search=1000, pre_reorder_num_neighbors=1000, final_num_neighbors=num_of_neighs,\n",
    "    )\n",
    "    P_neighbors, _ = P_searcher.search_batched(\n",
    "        query[:, embedding_size:], leaves_to_search=1000, pre_reorder_num_neighbors=1000, final_num_neighbors=num_of_neighs,\n",
    "    )\n",
    "    \n",
    "    fail = True\n",
    "    for q in range(num_of_neighs):\n",
    "        for p in range(num_of_neighs):\n",
    "            client = Client()\n",
    "            try:\n",
    "                if fail:\n",
    "                    site_pred = client.get(\n",
    "                        Qs_ordered_site[Q_neighbors[0][q]], load=True\n",
    "                    )[client.get(Ps_ordered_site[P_neighbors[0][p]])].id\n",
    "\n",
    "                    data['questions'][i]['answers'][0] = {\n",
    "                        'head': {'vars': ['result']},\n",
    "                        'results': {'bindings': []}\n",
    "                    }\n",
    "                    data['questions'][i]['answers'][0]['results']['bindings'].append(\n",
    "                        {'result': {'type': 'uri', 'value': f'http://www.wikidata.org/entity/{site_pred}'}}\n",
    "                    )\n",
    "                    \n",
    "                    data['questions'][i]['query'] = {\n",
    "                        \"sparql\": \"SELECT DISTINCT ?o1 WHERE { <http://www.wikidata.org/entity/\" + \\\n",
    "                        Qs_ordered_site[Q_neighbors[0][q]] + \">  <http://www.wikidata.org/prop/direct/\" + \\\n",
    "                        Ps_ordered_site[P_neighbors[0][p]] + \">  ?o1 .  }\"\n",
    "                    }\n",
    "                    fail = False\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if fail:\n",
    "            data['questions'][i]['answers'] = [{'head': {}, 'boolean': True}]\n",
    "            \n",
    "            data['questions'][i]['query'] = {\n",
    "                \"sparql\": \"SELECT DISTINCT ?o1 WHERE { <http://www.wikidata.org/entity/\" + \\\n",
    "                Qs_ordered_site[Q_neighbors[0][0]] + \">  <http://www.wikidata.org/prop/direct/\" + \\\n",
    "                Ps_ordered_site[P_neighbors[0][0]] + \">  ?o1 .  }\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_PATH = './submissions'\n",
    "SUBMIT_NAME = '{}.json'.format(SAVE_NAME.split('.')[0])\n",
    "\n",
    "with open(os.path.join(SUBMIT_PATH, SUBMIT_NAME), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link corresponed to this submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link https://gerbil-qa.aksw.org/gerbil/experiment?id=202205210032 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
